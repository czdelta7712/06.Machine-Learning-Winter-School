{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MusicGeneratorPianoPublic_GRU+ATTN.ipynb","provenance":[{"file_id":"https://github.com/haryoa/note_music_generator/blob/master/MusicGeneratorPianoColab.ipynb","timestamp":1579153495259}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"514520a3c85b4b5fbba2c124d6ead91c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_7169cf3637b84de3b5a1c653305c6b9d","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_9a3316f4408643f29fe6864db5bbc316","IPY_MODEL_ff1648e8d94543628f1e326093ac8866"]}},"7169cf3637b84de3b5a1c653305c6b9d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9a3316f4408643f29fe6864db5bbc316":{"model_module":"@jupyter-widgets/controls","model_name":"IntProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_7f546b9c1da5432f8bff6611f39295cf","_dom_classes":[],"description":"","_model_name":"IntProgressModel","bar_style":"success","max":3,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":3,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_02ebc97fa5dc42f08c9f89d0063ae831"}},"ff1648e8d94543628f1e326093ac8866":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_5a81dc2faf8540bda4103e735e2f4671","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100% 3/3 [00:02&lt;00:00,  1.06s/it]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8e711dd23eb74e75b7921247b78331bb"}},"7f546b9c1da5432f8bff6611f39295cf":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"02ebc97fa5dc42f08c9f89d0063ae831":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5a81dc2faf8540bda4103e735e2f4671":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"8e711dd23eb74e75b7921247b78331bb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b907344f038b4ae8b1b6ff298f93df69":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_54a9a863f364452fabf97b313267e8b3","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_4e484be362d740539dea0ab221ecbfb1","IPY_MODEL_64561c127a544cacae4dd020e1b1bcd4"]}},"54a9a863f364452fabf97b313267e8b3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4e484be362d740539dea0ab221ecbfb1":{"model_module":"@jupyter-widgets/controls","model_name":"IntProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_ff42832d0d134b43a10bf3978b01c83b","_dom_classes":[],"description":"genrt","_model_name":"IntProgressModel","bar_style":"success","max":1000,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1000,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_fddfcf88978d4fd0b5f543de8b415669"}},"64561c127a544cacae4dd020e1b1bcd4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_3054c93c9cee4a3ca1aa17c83bed2322","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100% 1000/1000 [00:39&lt;00:00, 25.17it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3092267afbe5478a9ef24003939d87b7"}},"ff42832d0d134b43a10bf3978b01c83b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"fddfcf88978d4fd0b5f543de8b415669":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3054c93c9cee4a3ca1aa17c83bed2322":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"3092267afbe5478a9ef24003939d87b7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9059e2abc73a4f7495a313d07a9884be":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_47dd223b9daa4b6f88e6b8d4d376e042","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_7feea96db3bc4faabccc1e39e67a4b7f","IPY_MODEL_df8f8f52e51543fbafe69cab493bcf9c"]}},"47dd223b9daa4b6f88e6b8d4d376e042":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7feea96db3bc4faabccc1e39e67a4b7f":{"model_module":"@jupyter-widgets/controls","model_name":"IntProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_e7ceb36ae1d74015b7fb6d76ffe65e3f","_dom_classes":[],"description":"genrt","_model_name":"IntProgressModel","bar_style":"danger","max":500,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":29,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3eabe338b9b84a8880c4acb1592e363b"}},"df8f8f52e51543fbafe69cab493bcf9c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_3d6808c7802f4c248fb14f4990949c8e","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"  6% 29/500 [00:01&lt;00:18, 25.41it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1d7b5ab59a134cba80c4e2fc4e1e2c61"}},"e7ceb36ae1d74015b7fb6d76ffe65e3f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"3eabe338b9b84a8880c4acb1592e363b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3d6808c7802f4c248fb14f4990949c8e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"1d7b5ab59a134cba80c4e2fc4e1e2c61":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"GtOla2Bc-hgJ","colab_type":"text"},"source":["Generate Piano Instrumental Music by Using Deep Learning\n","Generate Piano Music “♪♪♪” step by step by experimenting Tensorflow v2.0 Alpha\n","\n","2019.3\n","\n","https://towardsdatascience.com/generate-piano-instrumental-music-by-using-deep-learning-80ac35cdbd2e\n","\n","This is Modification from Original, Adjust and check \"seq_len\"\n","\n","Original Colab\n","https://colab.research.google.com/github/haryoa/note_music_generator/blob/master/MusicGeneratorPianoColab.ipynb#scrollTo=-aqumPylvXq0"]},{"cell_type":"code","metadata":{"id":"uMzP7jHiMi1S","colab_type":"code","outputId":"480a4666-d798-49d7-d47c-f87121fae051","executionInfo":{"status":"ok","timestamp":1582637942399,"user_tz":-540,"elapsed":54473,"user":{"displayName":"SUNGTAE CHI","photoUrl":"","userId":"10742170443466394516"}},"colab":{"base_uri":"https://localhost:8080/","height":955}},"source":["!pip install tensorflow==2.0.0-alpha0 \n","\n","import tensorflow as tf\n","from tensorflow.keras import backend as K\n","import glob\n","import random\n","import pretty_midi\n","import IPython\n","import numpy as np\n","from tqdm import tnrange, tqdm_notebook, tqdm\n","from random import shuffle, seed\n","import numpy as np\n","from tensorflow.keras.losses import sparse_categorical_crossentropy\n","from tensorflow.keras.optimizers import Nadam\n","import numpy as np\n","from numpy.random import choice\n","import pickle\n","import matplotlib.pyplot as plt\n","\n","import unicodedata\n","import re\n","import numpy as np\n","import os\n","import io\n","import time\n","\n","def piano_roll_to_pretty_midi(piano_roll, fs=100, program=0):\n","    '''Convert a Piano Roll array into a PrettyMidi object\n","     with a single instrument.\n","    Parameters\n","    ----------\n","    piano_roll : np.ndarray, shape=(128,frames), dtype=int\n","        Piano roll of one instrument\n","    fs : int\n","        Sampling frequency of the columns, i.e. each column is spaced apart\n","        by ``1./fs`` seconds.\n","    program : int\n","        The program number of the instrument.\n","    Returns\n","    -------\n","    midi_object : pretty_midi.PrettyMIDI\n","        A pretty_midi.PrettyMIDI class instance describing\n","        the piano roll.\n","    '''\n","    notes, frames = piano_roll.shape\n","    pm = pretty_midi.PrettyMIDI()\n","    instrument = pretty_midi.Instrument(program=program)\n","\n","    # pad 1 column of zeros so we can acknowledge inital and ending events\n","    piano_roll = np.pad(piano_roll, [(0, 0), (1, 1)], 'constant')\n","\n","    # use changes in velocities to find note on / note off events\n","    velocity_changes = np.nonzero(np.diff(piano_roll).T)\n","\n","    # keep track on velocities and note on times\n","    prev_velocities = np.zeros(notes, dtype=int)\n","    note_on_time = np.zeros(notes)\n","\n","    for time, note in zip(*velocity_changes):\n","        # use time + 1 because of padding above\n","        velocity = piano_roll[note, time + 1]\n","        time = time / fs\n","        if velocity > 0:\n","            if prev_velocities[note] == 0:\n","                note_on_time[note] = time\n","                prev_velocities[note] = velocity\n","        else:\n","            pm_note = pretty_midi.Note(\n","                velocity=prev_velocities[note],\n","                pitch=note,\n","                start=note_on_time[note],\n","                end=time)\n","            instrument.notes.append(pm_note)\n","            prev_velocities[note] = 0\n","    pm.instruments.append(instrument)\n","    return pm\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting tensorflow==2.0.0-alpha0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/39/f99185d39131b8333afcfe1dcdb0629c2ffc4ecfb0e4c14ca210d620e56c/tensorflow-2.0.0a0-cp36-cp36m-manylinux1_x86_64.whl (79.9MB)\n","\u001b[K     |████████████████████████████████| 79.9MB 59kB/s \n","\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (1.17.5)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (0.9.0)\n","Collecting tb-nightly<1.14.0a20190302,>=1.14.0a20190301\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/51/aa1d756644bf4624c03844115e4ac4058eff77acd786b26315f051a4b195/tb_nightly-1.14.0a20190301-py3-none-any.whl (3.0MB)\n","\u001b[K     |████████████████████████████████| 3.0MB 51.0MB/s \n","\u001b[?25hRequirement already satisfied: google-pasta>=0.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (0.1.8)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (1.0.8)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (1.12.0)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (0.34.2)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (0.8.1)\n","Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (0.2.2)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (1.1.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (1.1.0)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (1.27.1)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (3.10.0)\n","Collecting tf-estimator-nightly<1.14.0.dev2019030116,>=1.14.0.dev2019030115\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/82/f16063b4eed210dc2ab057930ac1da4fbe1e91b7b051a6c8370b401e6ae7/tf_estimator_nightly-1.14.0.dev2019030115-py2.py3-none-any.whl (411kB)\n","\u001b[K     |████████████████████████████████| 419kB 63.6MB/s \n","\u001b[?25hRequirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow==2.0.0-alpha0) (1.0.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow==2.0.0-alpha0) (3.2.1)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==2.0.0-alpha0) (2.8.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==2.0.0-alpha0) (45.1.0)\n","Installing collected packages: tb-nightly, tf-estimator-nightly, tensorflow\n","  Found existing installation: tensorflow 1.15.0\n","    Uninstalling tensorflow-1.15.0:\n","      Successfully uninstalled tensorflow-1.15.0\n","Successfully installed tb-nightly-1.14.0a20190301 tensorflow-2.0.0a0 tf-estimator-nightly-1.14.0.dev2019030115\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"slsEe0Kz2hyR","colab_type":"code","outputId":"305128e1-30da-49e0-ad9e-d214c4ea31d8","executionInfo":{"status":"ok","timestamp":1582637978965,"user_tz":-540,"elapsed":87408,"user":{"displayName":"SUNGTAE CHI","photoUrl":"","userId":"10742170443466394516"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"O7jtdhRooTIe","colab_type":"text"},"source":["# MIDI File data download - Not needed. I change it to my own."]},{"cell_type":"code","metadata":{"id":"-aqumPylvXq0","colab_type":"code","colab":{}},"source":["#!wget https://storage.googleapis.com/magentadata/datasets/maestro/v1.0.0/maestro-v1.0.0-midi.zip"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Maf04wQGPCPy","colab_type":"code","colab":{}},"source":["#!unzip maestro-v1.0.0-midi.zip"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T8vmvNgUwY8C","colab_type":"text"},"source":["# Get All Midi Files"]},{"cell_type":"code","metadata":{"id":"OaAywDhmO2Nr","colab_type":"code","colab":{}},"source":["def get_list_midi(folder = '/content/drive/My Drive/4thSchubertATTN/*.mid', seed_int = 4):\n","  \"\"\"Get the list of all midi file in the folders\n","  \n","  Parameters\n","  ==========\n","  folder : str\n","    The midi folder.\n","  seed_int : int\n","    the random seed.\n","  \n","  Returns\n","  =======\n","  The midi files\n","  \n","  \"\"\"\n","  list_all_midi = glob.glob(folder)\n","  seed(seed_int)\n","  shuffle(list_all_midi)\n","  return list_all_midi\n","\n","list_all_midi = get_list_midi()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-BHFcA4awsq9","colab_type":"text"},"source":["# Prepare some functions\n","See its documentation to see the function"]},{"cell_type":"code","metadata":{"id":"9XE-Om6DP15C","colab_type":"code","colab":{}},"source":["class NoteTokenizer:\n","    \n","    def __init__(self):\n","      self.notes_to_index = {}\n","      self.index_to_notes = {}\n","      self.num_of_word = 0\n","      self.unique_word = 0\n","      self.notes_freq = {}\n","        \n","    def transform(self,list_array):\n","      \"\"\" Transform a list of note in string into index.\n","      \n","      Parameters\n","      ==========\n","      list_array : list\n","        list of note in string format\n","      \n","      Returns\n","      =======\n","      The transformed list in numpy array.\n","      \n","      \"\"\"\n","      transformed_list = []\n","      for instance in list_array:\n","          transformed_list.append([self.notes_to_index[note] for note in instance])\n","      return np.array(transformed_list, dtype=np.int32)\n"," \n","    def partial_fit(self, notes):\n","        \"\"\" Partial fit on the dictionary of the tokenizer\n","        \n","        Parameters\n","        ==========\n","        notes : list of notes\n","        \n","        \"\"\"\n","        for note in notes:\n","            note_str = ','.join(str(a) for a in note)\n","            if note_str in self.notes_freq:\n","                self.notes_freq[note_str] += 1\n","                self.num_of_word += 1\n","            else:\n","                self.notes_freq[note_str] = 1\n","                self.unique_word += 1\n","                self.num_of_word += 1\n","                self.notes_to_index[note_str], self.index_to_notes[self.unique_word] = self.unique_word, note_str\n","            \n","    def add_new_note(self, note):\n","        \"\"\" Add a new note into the dictionary\n","\n","        Parameters\n","        ==========\n","        note : str\n","          a new note who is not in dictionary.  \n","\n","        \"\"\"\n","        assert note not in self.notes_to_index\n","        self.unique_word += 1\n","        self.notes_to_index[note], self.index_to_notes[self.unique_word] = self.unique_word, note\n","        \n","def generate_batch_song(list_all_midi, batch_music=16, start_index=0, fs=30, seq_len=100, use_tqdm=False):\n","    \"\"\"\n","    Generate Batch music that will be used to be input and output of the neural network\n","    \n","    Parameters\n","    ==========\n","    list_all_midi : list\n","      List of midi files\n","    batch_music : int\n","      A number of music in one batch\n","    start_index : int\n","      The start index to be batched in list_all_midi\n","    fs : int\n","      Sampling frequency of the columns, i.e. each column is spaced apart\n","        by ``1./fs`` seconds.\n","    seq_len : int\n","      The sequence length of the music to be input of neural network\n","    use_tqdm : bool\n","      Whether to use tqdm or not in the function\n","    \n","    Returns\n","    =======\n","    Tuple of input and target neural network\n","    \n","    \"\"\"\n","    \n","    assert len(list_all_midi) >= batch_music\n","    dict_time_notes = generate_dict_time_notes(list_all_midi, batch_music, start_index, fs, use_tqdm=use_tqdm)\n","    \n","    list_musics = process_notes_in_song(dict_time_notes, seq_len)\n","    collected_list_input, collected_list_target = [], []\n","     \n","    for music in list_musics:\n","        list_training, list_target = generate_input_and_target(music, seq_len)\n","        collected_list_input += list_training\n","        collected_list_target += list_target\n","    return collected_list_input, collected_list_target\n","\n","def generate_dict_time_notes(list_all_midi, batch_song = 16, start_index=0, fs=100, use_tqdm=True):\n","    \"\"\" Generate map (dictionary) of music ( in index ) to piano_roll (in np.array)\n","\n","    Parameters\n","    ==========\n","    list_all_midi : list\n","        List of midi files\n","    batch_music : int\n","      A number of music in one batch\n","    start_index : int\n","      The start index to be batched in list_all_midi\n","    fs : int\n","      Sampling frequency of the columns, i.e. each column is spaced apart\n","        by ``1./fs`` seconds.\n","    use_tqdm : bool\n","      Whether to use tqdm or not in the function\n","\n","    Returns\n","    =======\n","    dictionary of music to piano_roll (in np.array)\n","\n","    \"\"\"\n","    assert len(list_all_midi) >= batch_song\n","    \n","    dict_time_notes = {}\n","    process_tqdm_midi = tqdm_notebook(\n","        range(start_index, \n","              min(start_index + batch_song, \n","                  len(list_all_midi))))  if use_tqdm else range(start_index,  \n","                                                                min(start_index + batch_song, len(list_all_midi)))\n","    for i in process_tqdm_midi:\n","        midi_file_name = list_all_midi[i]\n","        if use_tqdm:\n","            process_tqdm_midi.set_description(\"Processing {}\".format(midi_file_name))\n","        try: # Handle exception on malformat MIDI files\n","            midi_pretty_format = pretty_midi.PrettyMIDI(midi_file_name)\n","          # piano_midi = midi_pretty_format.instruments[0] # Get the piano channels\n","            piano_midi = midi_pretty_format # All Channel, in case of Merged not Piano channel\n","            piano_roll = piano_midi.get_piano_roll(fs=fs)\n","            dict_time_notes[i] = piano_roll\n","        except Exception as e:\n","            print(e)\n","            print(\"broken file : {}\".format(midi_file_name))\n","            pass\n","    return dict_time_notes\n","\n","def generate_input_and_target(dict_keys_time, seq_len=100):\n","    \"\"\" Generate input and the target of our deep learning for one music.\n","    \n","    Parameters\n","    ==========\n","    dict_keys_time : dict\n","      Dictionary of timestep and notes\n","    seq_len : int\n","      The length of the sequence\n","      \n","    Returns\n","    =======\n","    Tuple of list of input and list of target of neural network.\n","    \n","       \n","    \"\"\"\n","    # Get the start time and end time\n","    start_time, end_time = list(dict_keys_time.keys())[0], list(dict_keys_time.keys())[-1]\n","    list_training, list_target = [], []\n","    for index_enum, time in enumerate(range(start_time, end_time)):\n","        list_append_training, list_append_target = [], []\n","        start_iterate = 0\n","        flag_target_append = False # flag to append the test list\n","        if index_enum < seq_len:\n","            start_iterate = seq_len - index_enum - 1\n","            for i in range(start_iterate): # add 'e' to the seq list. \n","                list_append_training.append('e')\n","                flag_target_append = True\n","\n","        for i in range(start_iterate,seq_len):\n","            index_enum = time - (seq_len - i - 1)\n","            if index_enum in dict_keys_time:\n","                list_append_training.append(','.join(str(x) for x in dict_keys_time[index_enum]))      \n","            else:\n","                list_append_training.append('e')\n","\n","        # add time + 1 to the list_append_target\n","        if time+1 in dict_keys_time:\n","            list_append_target.append(','.join(str(x) for x in dict_keys_time[time+1]))\n","        else:\n","            list_append_target.append('e')\n","        list_training.append(list_append_training)\n","        list_target.append(list_append_target)\n","    return list_training, list_target\n","\n","def process_notes_in_song(dict_time_notes, seq_len = 100):\n","    \"\"\"\n","    Iterate the dict of piano rolls into dictionary of timesteps and note played\n","    \n","    Parameters\n","    ==========\n","    dict_time_notes : dict\n","      dict contains index of music ( in index ) to piano_roll (in np.array)\n","    seq_len : int\n","      Length of the sequence\n","      \n","    Returns\n","    =======\n","    Dict of timesteps and note played\n","    \"\"\"\n","    list_of_dict_keys_time = []\n","    \n","    for key in dict_time_notes:\n","        sample = dict_time_notes[key]\n","        times = np.unique(np.where(sample > 0)[1])\n","        index = np.where(sample > 0)\n","        dict_keys_time = {}\n","\n","        for time in times:\n","            index_where = np.where(index[1] == time)\n","            notes = index[0][index_where]\n","            dict_keys_time[time] = notes\n","        list_of_dict_keys_time.append(dict_keys_time)\n","    return list_of_dict_keys_time\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TRiAye73zOfl","colab_type":"text"},"source":["# Sample 100 midi files from the datasets\n","These midi files will be used to train the neural network"]},{"cell_type":"code","metadata":{"id":"SS13kaENP3_d","colab_type":"code","colab":{}},"source":["# Get 200 midis file from the datasets.. \n","\n","sampled_200_midi = list_all_midi[0:3]  #0:3"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qGDSkeDlzeT_","colab_type":"text"},"source":["We create a map of note -> index here using NoteTokenizer that we've defined before.  \n","This object will be used to transform the list of notes to be ready for the input of Neural Network"]},{"cell_type":"code","metadata":{"id":"5gyoC7E6zdlI","colab_type":"code","outputId":"81a9ff66-634f-48d8-d7e3-1b447c15ebec","executionInfo":{"status":"ok","timestamp":1582637982234,"user_tz":-540,"elapsed":80229,"user":{"displayName":"SUNGTAE CHI","photoUrl":"","userId":"10742170443466394516"}},"colab":{"base_uri":"https://localhost:8080/","height":66,"referenced_widgets":["514520a3c85b4b5fbba2c124d6ead91c","7169cf3637b84de3b5a1c653305c6b9d","9a3316f4408643f29fe6864db5bbc316","ff1648e8d94543628f1e326093ac8866","7f546b9c1da5432f8bff6611f39295cf","02ebc97fa5dc42f08c9f89d0063ae831","5a81dc2faf8540bda4103e735e2f4671","8e711dd23eb74e75b7921247b78331bb"]}},"source":["# frame per second를 정의함. 저 아래 generate 때 또 나오는 데 그때 fs 와 지금 fs가 같아야 함. 다르면 에러\n","# frame per second는 Data곡의 빠르기 등을 고려해서 정해야 함. \n","# MM120의 4/4박자, 8분음표 길이는 1/16초이므로 이를 감안해서 계산, 즉 템포 \n","# Tempo x에서 4/4박, 4분음표(1beat) 1개 길이는 60/x, 이를 24개로 쪼개면(16분음표와 16분 셋잇단음 표현도 가능하도록) 기본단위.\n","# MM180일때 FPS = (180/60)/24 \n","batch = 1\n","start_index = 0\n","note_tokenizer = NoteTokenizer()\n","\n","for i in tqdm_notebook(range(len(sampled_200_midi))):\n","    dict_time_notes = generate_dict_time_notes(sampled_200_midi, batch_song=1, start_index=i, use_tqdm=False, fs=8) \n","    #<= frame per second 정의\n","    full_notes = process_notes_in_song(dict_time_notes)\n","    for note in full_notes:\n","        note_tokenizer.partial_fit(list(note.values()))\n","   "],"execution_count":7,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"514520a3c85b4b5fbba2c124d6ead91c","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RkdXvcqQWWQv","colab_type":"code","colab":{}},"source":["note_tokenizer.add_new_note('e') # Add empty notes"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7VXnQp3bXAKy","colab_type":"code","outputId":"5e160183-ec28-409a-c7fb-cd3529ab7602","executionInfo":{"status":"ok","timestamp":1582637983652,"user_tz":-540,"elapsed":79598,"user":{"displayName":"SUNGTAE CHI","photoUrl":"","userId":"10742170443466394516"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["unique_notes = note_tokenizer.unique_word\n","print(unique_notes)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["2822\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8XGrcuYUT8J3","colab_type":"text"},"source":["# Make Architecture\n","Now we will make the architecture.. \n","We will try to make the architecture as follow:\n","1. Embedding\n","2. LSTM\n","3. Self Head Attention\n","4. LSTM\n","5. Self Head Attention\n","6. Dense"]},{"cell_type":"code","metadata":{"id":"n0a4gE9QcFGx","colab_type":"code","colab":{}},"source":["seq_len = 100\n","EPOCHS = 60\n","BATCH_SONG = 4\n","BATCH_NNET_SIZE = 96 #원래는 얼마더라...\n","TOTAL_SONGS = len(sampled_200_midi)\n","FRAME_PER_SECOND = 8 # 8\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W3W60QHdVQF1","colab_type":"text"},"source":["We will use the code from here for self-attention  \n","https://github.com/CyberZHG/keras-self-attention/blob/master/keras_self_attention/seq_self_attention.py"]},{"cell_type":"code","metadata":{"id":"5TQW22YOVUVx","colab_type":"code","colab":{}},"source":["class SeqSelfAttention(tf.keras.layers.Layer):\n","\n","    ATTENTION_TYPE_ADD = 'additive'\n","    ATTENTION_TYPE_MUL = 'multiplicative'\n","\n","    def __init__(self,\n","                 units=64,\n","                 attention_width=None,\n","                 attention_type=ATTENTION_TYPE_ADD,\n","                 return_attention=False,\n","                 history_only=False,\n","                 kernel_initializer='glorot_normal',\n","                 bias_initializer='zeros',\n","                 kernel_regularizer=None,\n","                 bias_regularizer=None,\n","                 kernel_constraint=None,\n","                 bias_constraint=None,\n","                 use_additive_bias=True,\n","                 use_attention_bias=True,\n","                 attention_activation=None,\n","                 attention_regularizer_weight=0.0,\n","                 **kwargs):\n","      \n","      # originally units=32\n","      \n","        \"\"\"Layer initialization.\n","        For additive attention, see: https://arxiv.org/pdf/1806.01264.pdf\n","        :param units: The dimension of the vectors that used to calculate the attention weights.\n","        :param attention_width: The width of local attention.\n","        :param attention_type: 'additive' or 'multiplicative'.\n","        :param return_attention: Whether to return the attention weights for visualization.\n","        :param history_only: Only use historical pieces of data.\n","        :param kernel_initializer: The initializer for weight matrices.\n","        :param bias_initializer: The initializer for biases.\n","        :param kernel_regularizer: The regularization for weight matrices.\n","        :param bias_regularizer: The regularization for biases.\n","        :param kernel_constraint: The constraint for weight matrices.\n","        :param bias_constraint: The constraint for biases.\n","        :param use_additive_bias: Whether to use bias while calculating the relevance of inputs features\n","                                  in additive mode.\n","        :param use_attention_bias: Whether to use bias while calculating the weights of attention.\n","        :param attention_activation: The activation used for calculating the weights of attention.\n","        :param attention_regularizer_weight: The weights of attention regularizer.\n","        :param kwargs: Parameters for parent class.\n","        \"\"\"\n","        self.supports_masking = True\n","        self.units = units\n","        self.attention_width = attention_width\n","        self.attention_type = attention_type\n","        self.return_attention = return_attention\n","        self.history_only = history_only\n","        if history_only and attention_width is None:\n","            self.attention_width = int(1e9)\n","\n","        self.use_additive_bias = use_additive_bias\n","        self.use_attention_bias = use_attention_bias\n","        self.kernel_initializer = tf.keras.initializers.get(kernel_initializer)\n","        self.bias_initializer = tf.keras.initializers.get(bias_initializer)\n","        self.kernel_regularizer = tf.keras.regularizers.get(kernel_regularizer)\n","        self.bias_regularizer = tf.keras.regularizers.get(bias_regularizer)\n","        self.kernel_constraint = tf.keras.constraints.get(kernel_constraint)\n","        self.bias_constraint = tf.keras.constraints.get(bias_constraint)\n","        self.attention_activation = tf.keras.activations.get(attention_activation)\n","        self.attention_regularizer_weight = attention_regularizer_weight\n","        self._backend = tf.keras.backend.backend()\n","\n","        if attention_type == SeqSelfAttention.ATTENTION_TYPE_ADD:\n","            self.Wx, self.Wt, self.bh = None, None, None\n","            self.Wa, self.ba = None, None\n","        elif attention_type == SeqSelfAttention.ATTENTION_TYPE_MUL:\n","            self.Wa, self.ba = None, None\n","        else:\n","            raise NotImplementedError('No implementation for attention type : ' + attention_type)\n","\n","        super(SeqSelfAttention, self).__init__(**kwargs)\n","\n","    def get_config(self):\n","        config = {\n","            'units': self.units,\n","            'attention_width': self.attention_width,\n","            'attention_type': self.attention_type,\n","            'return_attention': self.return_attention,\n","            'history_only': self.history_only,\n","            'use_additive_bias': self.use_additive_bias,\n","            'use_attention_bias': self.use_attention_bias,\n","            'kernel_initializer': tf.keras.regularizers.serialize(self.kernel_initializer),\n","            'bias_initializer': tf.keras.regularizers.serialize(self.bias_initializer),\n","            'kernel_regularizer': tf.keras.regularizers.serialize(self.kernel_regularizer),\n","            'bias_regularizer': tf.keras.regularizers.serialize(self.bias_regularizer),\n","            'kernel_constraint': tf.keras.constraints.serialize(self.kernel_constraint),\n","            'bias_constraint': tf.keras.constraints.serialize(self.bias_constraint),\n","            'attention_activation': tf.keras.activations.serialize(self.attention_activation),\n","            'attention_regularizer_weight': self.attention_regularizer_weight,\n","        }\n","        base_config = super(SeqSelfAttention, self).get_config()\n","        return dict(list(base_config.items()) + list(config.items()))\n","\n","    def build(self, input_shape):\n","        if isinstance(input_shape, list):\n","            input_shape = input_shape[0]\n","        if self.attention_type == SeqSelfAttention.ATTENTION_TYPE_ADD:\n","            self._build_additive_attention(input_shape)\n","        elif self.attention_type == SeqSelfAttention.ATTENTION_TYPE_MUL:\n","            self._build_multiplicative_attention(input_shape)\n","        super(SeqSelfAttention, self).build(input_shape)\n","\n","    def _build_additive_attention(self, input_shape):\n","        feature_dim = input_shape[2]\n","\n","        self.Wt = self.add_weight(shape=(feature_dim, self.units),\n","                                  name='{}_Add_Wt'.format(self.name),\n","                                  initializer=self.kernel_initializer,\n","                                  regularizer=self.kernel_regularizer,\n","                                  constraint=self.kernel_constraint)\n","        self.Wx = self.add_weight(shape=(feature_dim, self.units),\n","                                  name='{}_Add_Wx'.format(self.name),\n","                                  initializer=self.kernel_initializer,\n","                                  regularizer=self.kernel_regularizer,\n","                                  constraint=self.kernel_constraint)\n","        if self.use_additive_bias:\n","            self.bh = self.add_weight(shape=(self.units,),\n","                                      name='{}_Add_bh'.format(self.name),\n","                                      initializer=self.bias_initializer,\n","                                      regularizer=self.bias_regularizer,\n","                                      constraint=self.bias_constraint)\n","\n","        self.Wa = self.add_weight(shape=(self.units, 1),\n","                                  name='{}_Add_Wa'.format(self.name),\n","                                  initializer=self.kernel_initializer,\n","                                  regularizer=self.kernel_regularizer,\n","                                  constraint=self.kernel_constraint)\n","        if self.use_attention_bias:\n","            self.ba = self.add_weight(shape=(1,),\n","                                      name='{}_Add_ba'.format(self.name),\n","                                      initializer=self.bias_initializer,\n","                                      regularizer=self.bias_regularizer,\n","                                      constraint=self.bias_constraint)\n","\n","    def _build_multiplicative_attention(self, input_shape):\n","        feature_dim = input_shape[2]\n","\n","        self.Wa = self.add_weight(shape=(feature_dim, feature_dim),\n","                                  name='{}_Mul_Wa'.format(self.name),\n","                                  initializer=self.kernel_initializer,\n","                                  regularizer=self.kernel_regularizer,\n","                                  constraint=self.kernel_constraint)\n","        if self.use_attention_bias:\n","            self.ba = self.add_weight(shape=(1,),\n","                                      name='{}_Mul_ba'.format(self.name),\n","                                      initializer=self.bias_initializer,\n","                                      regularizer=self.bias_regularizer,\n","                                      constraint=self.bias_constraint)\n","\n","    def call(self, inputs, mask=None, **kwargs):\n","        if isinstance(inputs, list):\n","            inputs, positions = inputs\n","            positions = K.cast(positions, 'int32')\n","            mask = mask[1]\n","        else:\n","            positions = None\n","\n","        input_len = K.shape(inputs)[1]\n","\n","        if self.attention_type == SeqSelfAttention.ATTENTION_TYPE_ADD:\n","            e = self._call_additive_emission(inputs)\n","        elif self.attention_type == SeqSelfAttention.ATTENTION_TYPE_MUL:\n","            e = self._call_multiplicative_emission(inputs)\n","\n","        if self.attention_activation is not None:\n","            e = self.attention_activation(e)\n","        e = K.exp(e - K.max(e, axis=-1, keepdims=True))\n","        if self.attention_width is not None:\n","            ones = tf.ones((input_len, input_len))\n","            if self.history_only:\n","                local = tf.linalg.band_part(\n","                    ones,\n","                    K.minimum(input_len, self.attention_width - 1),\n","                    0,\n","                )\n","            else:\n","                local = tf.linalg.band_part(\n","                    ones,\n","                    K.minimum(input_len, self.attention_width // 2),\n","                    K.minimum(input_len, (self.attention_width - 1) // 2),\n","                )\n","            e = e * K.expand_dims(local, 0)\n","        if mask is not None:\n","            mask = K.cast(mask, K.floatx())\n","            mask = K.expand_dims(mask)\n","            e = K.permute_dimensions(K.permute_dimensions(e * mask, (0, 2, 1)) * mask, (0, 2, 1))\n","\n","        # a_{t} = \\text{softmax}(e_t)\n","        s = K.sum(e, axis=-1)\n","        s = K.tile(K.expand_dims(s, axis=-1), K.stack([1, 1, input_len]))\n","        a = e / (s + K.epsilon())\n","\n","        # l_t = \\sum_{t'} a_{t, t'} x_{t'}\n","        v = K.batch_dot(a, inputs)\n","        if self.attention_regularizer_weight > 0.0:\n","            self.add_loss(self._attention_regularizer(a))\n","\n","        if positions is not None:\n","            pos_num = K.shape(positions)[1]\n","            batch_indices = K.tile(K.expand_dims(K.arange(K.shape(inputs)[0]), axis=-1), K.stack([1, pos_num]))\n","            pos_indices = K.stack([batch_indices, positions], axis=-1)\n","            v = tf.gather_nd(v, pos_indices)\n","            a = tf.gather_nd(a, pos_indices)\n","\n","        if self.return_attention:\n","            return [v, a]\n","        return v\n","\n","    def _call_additive_emission(self, inputs):\n","        input_shape = K.shape(inputs)\n","        batch_size, input_len = input_shape[0], input_shape[1]\n","\n","        # h_{t, t'} = \\tanh(x_t^T W_t + x_{t'}^T W_x + b_h)\n","        q, k = K.dot(inputs, self.Wt), K.dot(inputs, self.Wx)\n","        q = K.tile(K.expand_dims(q, 2), K.stack([1, 1, input_len, 1]))\n","        k = K.tile(K.expand_dims(k, 1), K.stack([1, input_len, 1, 1]))\n","        if self.use_additive_bias:\n","            h = K.tanh(q + k + self.bh)\n","        else:\n","            h = K.tanh(q + k)\n","\n","        # e_{t, t'} = W_a h_{t, t'} + b_a\n","        if self.use_attention_bias:\n","            e = K.reshape(K.dot(h, self.Wa) + self.ba, (batch_size, input_len, input_len))\n","        else:\n","            e = K.reshape(K.dot(h, self.Wa), (batch_size, input_len, input_len))\n","        return e\n","\n","    def _call_multiplicative_emission(self, inputs):\n","        # e_{t, t'} = x_t^T W_a x_{t'} + b_a\n","        e = K.batch_dot(K.dot(inputs, self.Wa), K.permute_dimensions(inputs, (0, 2, 1)))\n","        if self.use_attention_bias:\n","            e = e + self.ba\n","        return e\n","\n","    def compute_output_shape(self, input_shape):\n","        if isinstance(input_shape, list):\n","            input_shape, pos_shape = input_shape\n","            output_shape = (input_shape[0], pos_shape[1], input_shape[2])\n","        else:\n","            output_shape = input_shape\n","        if self.return_attention:\n","            attention_shape = (input_shape[0], output_shape[1], input_shape[1])\n","            return [output_shape, attention_shape]\n","        return output_shape\n","\n","    def compute_mask(self, inputs, mask=None):\n","        if isinstance(inputs, list):\n","            mask = mask[1]\n","        if self.return_attention:\n","            return [mask, None]\n","        return mask\n","\n","    def _attention_regularizer(self, attention):\n","        batch_size = K.cast(K.shape(attention)[0], K.floatx())\n","        input_len = K.shape(attention)[-1]\n","        return self.attention_regularizer_weight * K.sum(K.square(K.batch_dot(\n","            attention,\n","            K.permute_dimensions(attention, (0, 2, 1))) - tf.eye(input_len))) / batch_size\n","\n","    @staticmethod\n","    def get_custom_objects():\n","      return {'SeqSelfAttention': SeqSelfAttention}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VounAZqR9MWH","colab_type":"code","colab":{}},"source":["# Attention Length를 조절하는 곳 (default attention_width=50)\n","\n","def create_model(seq_len, unique_notes, dropout=0.3, output_emb=160, rnn_unit=128, dense_unit=64):\n","  # seq_len, unique_notes, dropout=0.3, output_emb=100, rnn_unit=128, dense_unit=64)\n","\n","  inputs = tf.keras.layers.Input(shape=(seq_len,))\n","  embedding = tf.keras.layers.Embedding(input_dim=unique_notes+1, output_dim=output_emb, input_length=seq_len)(inputs)\n","  forward_pass = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(rnn_unit, return_sequences=True))(embedding)\n","  forward_pass , att_vector = SeqSelfAttention(\n","      return_attention=True,\n","      attention_activation='sigmoid', \n","      attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,\n","      attention_width=50, \n","      kernel_regularizer=tf.keras.regularizers.l2(1e-4),\n","      bias_regularizer=tf.keras.regularizers.l1(1e-4),\n","      attention_regularizer_weight=1e-4,\n","  )(forward_pass)\n","  forward_pass = tf.keras.layers.Dropout(dropout)(forward_pass)\n","  forward_pass = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(rnn_unit, return_sequences=True))(forward_pass)\n","  forward_pass , att_vector2 = SeqSelfAttention(\n","      return_attention=True,\n","      attention_activation='sigmoid', \n","      attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,\n","      attention_width=50, \n","      kernel_regularizer=tf.keras.regularizers.l2(1e-4),\n","      bias_regularizer=tf.keras.regularizers.l1(1e-4),\n","      attention_regularizer_weight=1e-4,\n","  )(forward_pass)\n","  forward_pass = tf.keras.layers.Dropout(dropout)(forward_pass)\n","  forward_pass = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(rnn_unit))(forward_pass)\n","  forward_pass = tf.keras.layers.Dropout(dropout)(forward_pass)\n","  forward_pass = tf.keras.layers.Dense(dense_unit)(forward_pass)\n","  forward_pass = tf.keras.layers.LeakyReLU()(forward_pass)\n","  outputs = tf.keras.layers.Dense(unique_notes+1, activation = \"softmax\")(forward_pass)\n","\n","  model = tf.keras.Model(inputs=inputs, outputs=outputs, name='generate_scores_rnn')\n","  return model\n","\n","model = create_model(seq_len, unique_notes)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dUgxJaPQf757","colab_type":"code","outputId":"21a2d7e9-d587-4b72-dbfd-cb4a966d3725","executionInfo":{"status":"ok","timestamp":1582515814230,"user_tz":-540,"elapsed":688,"user":{"displayName":"SUNGTAE CHI","photoUrl":"","userId":"10742170443466394516"}},"colab":{"base_uri":"https://localhost:8080/","height":595}},"source":["model.summary()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Model: \"generate_scores_rnn\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_1 (InputLayer)         [(None, 100)]             0         \n","_________________________________________________________________\n","embedding (Embedding)        (None, 100, 160)          451680    \n","_________________________________________________________________\n","bidirectional (Bidirectional (None, 100, 256)          222720    \n","_________________________________________________________________\n","seq_self_attention (SeqSelfA [(None, 100, 256), (None, 65537     \n","_________________________________________________________________\n","dropout (Dropout)            (None, 100, 256)          0         \n","_________________________________________________________________\n","bidirectional_1 (Bidirection (None, 100, 256)          296448    \n","_________________________________________________________________\n","seq_self_attention_1 (SeqSel [(None, 100, 256), (None, 65537     \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 100, 256)          0         \n","_________________________________________________________________\n","bidirectional_2 (Bidirection (None, 256)               296448    \n","_________________________________________________________________\n","dropout_2 (Dropout)          (None, 256)               0         \n","_________________________________________________________________\n","dense (Dense)                (None, 64)                16448     \n","_________________________________________________________________\n","leaky_re_lu (LeakyReLU)      (None, 64)                0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 2823)              183495    \n","=================================================================\n","Total params: 1,598,313\n","Trainable params: 1,598,313\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rwGkN3KiiZ-H","colab_type":"code","colab":{}},"source":["tf.keras.utils.plot_model(model, '/content/drive/My Drive/4thSchubertATTN/model/this_model.png', show_shapes=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EmSzoDqpjxE5","colab_type":"text"},"source":["# Train"]},{"cell_type":"markdown","metadata":{"id":"wZ4L3mgtq6cB","colab_type":"text"},"source":["체크포인트 - [Tensorflow 2.0] 모델 저장하고 불러오기를 참조\n","\n","https://medium.com/@a.ydobon/%EC%83%88%EB%A1%9C%EC%9A%B4-%ED%85%90%EC%84%9C%ED%94%8C%EB%A1%9C%EC%9A%B02-0-%EB%A7%8C%EB%93%A0-%EB%AA%A8%EB%8D%B8%EC%9D%84-%EC%A0%80%EC%9E%A5%ED%95%98%EA%B3%A0-%EB%B6%88%EB%9F%AC%EC%98%A4%EA%B8%B0-5da506b59e13"]},{"cell_type":"code","metadata":{"id":"x0xN3s3yobJQ","colab_type":"code","colab":{}},"source":["import os\n","from keras.callbacks import ModelCheckpoint\n","\n","#callback 기능 새로 추가\n","\n","optimizer = Nadam()\n","\n","\n","#####체크포인트 방식 변경#####\n","\n","checkpoint_dir = '/content/drive/My Drive/4thSchubertATTN/training_checkpoint'\n","checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n","loss_fn = sparse_categorical_crossentropy\n","\n","checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n","                                 model=model)\n","status = checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n","\n","# Create a callback that saves the model's weights\n","cp_callback = tf.keras.callbacks.ModelCheckpoint(\n","    filepath=checkpoint_dir, monitor='val_loss',\n","    verbose=0, save_best_only=True, save_weights_only=True, mode='auto', period=1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XGng437qnIpY","colab_type":"code","colab":{}},"source":["class TrainModel:\n","\n","#  def __init__(self, epochs, note_tokenizer, sampled_200_midi, frame_per_second, \n","#               batch_nnet_size, batch_song, optimizer, checkpoint, loss_fn,\n","#               checkpoint_prefix, total_songs, model):\n","#### Call Back 기능 추가 ######\n","  def __init__(self, epochs, note_tokenizer, sampled_200_midi, frame_per_second, \n","               batch_nnet_size, batch_song, optimizer, checkpoint, loss_fn,\n","               checkpoint_prefix, total_songs, model, callbacks):    \n","\n","    self.epochs = epochs\n","    self.note_tokenizer = note_tokenizer\n","    self.sampled_200_midi = sampled_200_midi\n","    self.frame_per_second = frame_per_second\n","    self.batch_nnet_size = batch_nnet_size\n","    self.batch_song = batch_song\n","    self.optimizer = optimizer \n","    self.callbacks = callbacks   ## <==새로 추가하였음\n","    self.checkpoint = checkpoint\n","    self.loss_fn = loss_fn\n","    self.checkpoint_prefix = checkpoint_prefix\n","    self.total_songs = total_songs\n","    self.model = model\n","    \n","  def train(self):\n","    for epoch in tqdm_notebook(range(self.epochs),desc='epochs'):\n","      # for each epochs, we shufle the list of all the datasets\n","      shuffle(self.sampled_200_midi)\n","      loss_total = 0\n","      steps = 0\n","      steps_nnet = 0\n","\n","      # We will iterate all songs by self.song_size\n","      for i in tqdm_notebook(range(0,self.total_songs, self.batch_song), desc='MUSIC'):\n","\n","        steps += 1\n","        inputs_nnet_large, outputs_nnet_large = generate_batch_song(\n","            self.sampled_200_midi, self.batch_song, start_index=i, fs=self.frame_per_second, \n","            seq_len=seq_len, use_tqdm=False) # We use the function that have been defined here\n","        inputs_nnet_large = np.array(self.note_tokenizer.transform(inputs_nnet_large), dtype=np.int32)\n","        outputs_nnet_large = np.array(self.note_tokenizer.transform(outputs_nnet_large), dtype=np.int32)\n","\n","        index_shuffled = np.arange(start=0, stop=len(inputs_nnet_large))\n","        np.random.shuffle(index_shuffled)\n","\n","        for nnet_steps in tqdm_notebook(range(0,len(index_shuffled),self.batch_nnet_size)):\n","          steps_nnet += 1\n","          current_index = index_shuffled[nnet_steps:nnet_steps+self.batch_nnet_size]\n","          inputs_nnet, outputs_nnet = inputs_nnet_large[current_index], outputs_nnet_large[current_index]\n","          \n","          # To make sure no exception thrown by tensorflow on autograph\n","          if len(inputs_nnet) // self.batch_nnet_size != 1:\n","            break\n","          loss = self.train_step(inputs_nnet, outputs_nnet)\n","          loss_total += tf.math.reduce_sum(loss)\n","          if steps_nnet % 20 == 0:  #Epoch 1회당 20step 마다 sparseCrossEntropy 값 보여줌\n","#          if steps_nnet % 100 == 0:\n","           print(\"epochs {} | Steps {} | total loss : {}\".format(epoch + 1, steps_nnet, loss_total))\n","\n","      checkpoint.save(file_prefix = self.checkpoint_prefix)\n","      \n","\n","  @tf.function\n","  def train_step(self, inputs, targets):\n","    with tf.GradientTape() as tape:\n","      prediction = self.model(inputs)\n","      loss = self.loss_fn(targets, prediction)\n","    gradients = tape.gradient(loss, self.model.trainable_variables)\n","    self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n","    return loss"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LCAoHVQ11ASd","colab_type":"text"},"source":["# Train Here\n","Beware that the time of one epoch is around 1 hour in case of Original Dataset(seq_len=50, Batch_song=8, FPS=5)"]},{"cell_type":"code","metadata":{"id":"fIbHBr1trNUP","colab_type":"code","colab":{}},"source":["\n","seq_len =100\n","EPOCHS = 40\n","BATCH_SONG = 3 #3\n","#BATCH_NNET_SIZE = 72\n","BATCH_NNET_SIZE = 96\n","TOTAL_SONGS = len(sampled_200_midi)\n","FRAME_PER_SECOND = 8 #8\n","\n","##### callback기능 추가1 ######\n","#checkpoint.restore('/content/drive/My Drive/4thSchubertATTN/training_checkpoint/ckpt-50.index').assert_consumed()\n","checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)\n","status = checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n","\n","#train_class = TrainModel(EPOCHS, note_tokenizer, sampled_200_midi, FRAME_PER_SECOND,\n","#                  BATCH_NNET_SIZE, BATCH_SONG, optimizer, checkpoint, loss_fn,\n","#                  checkpoint_prefix, TOTAL_SONGS, model)\n","\n","##### callback기능 추가2 이게 맞는 건지 아닌지 모름 ######\n","train_class = TrainModel(EPOCHS, note_tokenizer, sampled_200_midi, FRAME_PER_SECOND,\n","                  BATCH_NNET_SIZE, BATCH_SONG, optimizer, checkpoint, loss_fn,\n","                  checkpoint_prefix, TOTAL_SONGS, model, callbacks=[cp_callback])\n","\n","train_class.train()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G0YZVrlj1PCH","colab_type":"text"},"source":["# Save the model as h5 and The Tokenizer"]},{"cell_type":"code","metadata":{"id":"9cYPiBIZ9pUI","colab_type":"code","colab":{}},"source":["# 분할 학습했다면 실제 총 epoch 횟수를 어림잡아서 입력\n","# epoch=170 \n","#model.save (\"saved-model-{epoch:02d}-{val_acc:.2f}.hdf5\")\n","model.save('/content/drive/My Drive/4thSchubertATTN/model/RMdata12_model_ep210_loss_0580.h5')\n","pickle.dump( note_tokenizer, open( '/content/drive/My Drive/4thSchubertATTN/model/RMdata12_model_ep210_loss_0580_tokenizer.p', \"wb\" ) )"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VCARY5DB2F3Y","colab_type":"text"},"source":["# Try To load the model and the Tokenizer"]},{"cell_type":"code","metadata":{"id":"jw1aCPCXxbwG","colab_type":"code","outputId":"f6af89c6-ee60-401a-b81e-3c10ea033fed","executionInfo":{"status":"ok","timestamp":1582632222083,"user_tz":-540,"elapsed":2001,"user":{"displayName":"SUNGTAE CHI","photoUrl":"","userId":"10742170443466394516"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["model = tf.keras.models.load_model(f'/content/drive/My Drive/4thSchubertATTN/model/RMdata12_model_ep210_loss_0580.h5', \n","                                   custom_objects=SeqSelfAttention.get_custom_objects())\n","note_tokenizer  = pickle.load( open( \"/content/drive/My Drive/4thSchubertATTN/model/RMdata12_model_ep210_loss_0580_tokenizer.p\", \"rb\" ) )\n","# note_tokenizer  = pickle.load( open( \"./tokenizer.p\", \"rb\" ) )"],"execution_count":15,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"oLtB11sJ2T2g","colab_type":"text"},"source":["# Generate Midi Files"]},{"cell_type":"markdown","metadata":{"id":"9J225qPUAbgT","colab_type":"text"},"source":["Definition : Random Note Generate"]},{"cell_type":"code","metadata":{"id":"WUubXZFWmz_l","colab_type":"code","colab":{}},"source":["def generate_from_random(unique_notes, seq_len=100):\n","  generate = np.random.randint(0,unique_notes,seq_len).tolist()\n","  return generate\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hSMdg9rHAhkr","colab_type":"text"},"source":["Definition : One Note Generate"]},{"cell_type":"code","metadata":{"id":"zbmcsW23ATDo","colab_type":"code","colab":{}},"source":["def generate_from_one_note(note_tokenizer, new_notes='35'):\n","  generate = [note_tokenizer.notes_to_index['e'] for i in range(119)]\n","  generate += [note_tokenizer.notes_to_index[new_notes]]\n","  return generate"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QtD6YQrVAg0I","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"auOai0dppfC2","colab_type":"code","colab":{}},"source":["def generate_notes(generate, model, unique_notes, max_generated=1000, seq_len=100):\n","  for i in tqdm_notebook(range(max_generated), desc='genrt'):\n","    test_input = np.array([generate])[:,i:i+seq_len]\n","    predicted_note = model.predict(test_input)\n","    random_note_pred = choice(unique_notes+1, 1, replace=False, p=predicted_note[0])\n","    generate.append(random_note_pred[0])\n","  return generate"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z13PBRKZqlKH","colab_type":"code","colab":{}},"source":["def write_midi_file_from_generated(generate, midi_file_name = \n","                                   \"/content/drive/My Drive/4thSchubertATTN/model/result.mid\", \n","                                   start_index=8, fs=8, max_generated=1000):   \n","                                   #start_index=50 <=== 첫 Random Note 갯수를 정해주는 것(>12)\n","  note_string = [note_tokenizer.index_to_notes[ind_note] for ind_note in generate]\n","  array_piano_roll = np.zeros((128,max_generated+1), dtype=np.int16)\n","  for index, note in enumerate(note_string[start_index:]):\n","    if note == 'e':\n","      pass\n","    else:\n","      splitted_note = note.split(',')\n","      for j in splitted_note:\n","        array_piano_roll[int(j),index] = 1\n","  generate_to_midi = piano_roll_to_pretty_midi(array_piano_roll, fs=fs)\n","  print(\"Tempo {}\".format(generate_to_midi.estimate_tempo()))\n","  for note in generate_to_midi.instruments[0].notes:\n","    note.velocity = 100\n","  generate_to_midi.write(midi_file_name)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kLZMvPdeBGo-","colab_type":"text"},"source":["Generating : Random Note _ Not Humantic\n","\n","fs 를 조절해서 곡의 템포를 조절, 학습 FPS8이면 생성fps는 6정도 했을때 BPM 154 나옴(8분음표 위주)"]},{"cell_type":"code","metadata":{"id":"-PJcnpf8mUKG","colab_type":"code","outputId":"531ca3c6-c080-413e-f4eb-b03473edcbde","executionInfo":{"status":"ok","timestamp":1582633173716,"user_tz":-540,"elapsed":41701,"user":{"displayName":"SUNGTAE CHI","photoUrl":"","userId":"10742170443466394516"}},"colab":{"base_uri":"https://localhost:8080/","height":83,"referenced_widgets":["b907344f038b4ae8b1b6ff298f93df69","54a9a863f364452fabf97b313267e8b3","4e484be362d740539dea0ab221ecbfb1","64561c127a544cacae4dd020e1b1bcd4","ff42832d0d134b43a10bf3978b01c83b","fddfcf88978d4fd0b5f543de8b415669","3054c93c9cee4a3ca1aa17c83bed2322","3092267afbe5478a9ef24003939d87b7"]}},"source":["max_generate = 1000\n","unique_notes = note_tokenizer.unique_word\n","seq_len=100 #100 \n","generate = generate_from_random(unique_notes, seq_len)\n","generate = generate_notes(generate, model, unique_notes, max_generate, seq_len)\n","##write_midi_file_from_generated(generate, \"random.mid\", start_index=seq_len-1, fs=8, max_generated = max_generate)\n","#write_midi_file_from_generated(generate, \n","#                               \"/content/drive/My Drive/4thSchubertATTN/modelTEST/random1000_fs10_8.mid\", \n","#                               start_index=seq_len-1, fs=8, max_generated = max_generate)\n","#loss_total = loss_total\n","fps=4\n","Epochs = 210\n","write_midi_file_from_generated(generate, \n","                               f\"/content/drive/My Drive/4thSchubertATTN/model/randomRM{max_generate}_SeqL{seq_len}_EP{Epochs}_FPS{FRAME_PER_SECOND}_fs{fps}loss0580.mid\", \n","                               start_index=seq_len-1, fs=fps, max_generated = max_generate)\n","# d 10진 정수 decimal integer => epoch:02d}-{loss:.4f} {loss_total:.1f}\n","# 실수를 넣을 때는 %f를 사용하며 고정 소수점 fixed point. 소수점 이하 자릿수를 지정하고 싶다면 다음과 같이 f 앞에 .(점\n","\n","# fs는 곡의 템포를 바꾸므로 Notation이 달라질 뿐 곡은 원래 특성대로 계속 생성됨\n"],"execution_count":21,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b907344f038b4ae8b1b6ff298f93df69","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=0, description='genrt', max=1000, style=ProgressStyle(description_width='init…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","Tempo 183.75000000000006\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"BUeENSd-BL7u","colab_type":"text"},"source":["Generating : One note\n","\n","fs 를 조절해서 곡의 템포를 조절, 학습 FPS8이면 생성fps4가 보기좋음 (8분음표 위주)"]},{"cell_type":"code","metadata":{"id":"WWr_tXdZQuYX","colab_type":"code","outputId":"fe5d0700-c6f4-47e4-a971-d81416646056","executionInfo":{"status":"error","timestamp":1582540840505,"user_tz":-540,"elapsed":1893,"user":{"displayName":"아르떼","photoUrl":"","userId":"02010724767235108618"}},"colab":{"base_uri":"https://localhost:8080/","height":444,"referenced_widgets":["9059e2abc73a4f7495a313d07a9884be","47dd223b9daa4b6f88e6b8d4d376e042","7feea96db3bc4faabccc1e39e67a4b7f","df8f8f52e51543fbafe69cab493bcf9c","e7ceb36ae1d74015b7fb6d76ffe65e3f","3eabe338b9b84a8880c4acb1592e363b","3d6808c7802f4c248fb14f4990949c8e","1d7b5ab59a134cba80c4e2fc4e1e2c61"]}},"source":["##### 갑자기 안된다######\n","\n","max_generate = 500\n","unique_notes = note_tokenizer.unique_word\n","seq_len=100\n","generate = generate_from_one_note(note_tokenizer, '72')\n","#generate = generate_from_one_note(note_tokenizer)\n","print(seq_len)\n","generate = generate_notes(generate, model, unique_notes, max_generate, seq_len)\n","fps=8\n","write_midi_file_from_generated(generate, \n","                               f\"/content/drive/My Drive/4thSchubertATTN/model/OneNote{max_generate}_SeqL{seq_len}_EP{EPOCHS}_FPS{FRAME_PER_SECOND}_fs{fps}Loss0580.mid\", \n","                               start_index=seq_len-1, fs=fps, max_generated = max_generate)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["100\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9059e2abc73a4f7495a313d07a9884be","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=0, description='genrt', max=500, style=ProgressStyle(description_width='initi…"]},"metadata":{"tags":[]}},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-98-000ff7b7402b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#generate = generate_from_one_note(note_tokenizer)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mgenerate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_notes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munique_notes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_generate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mfps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m write_midi_file_from_generated(generate, \n","\u001b[0;32m<ipython-input-95-eee2c90ca201>\u001b[0m in \u001b[0;36mgenerate_notes\u001b[0;34m(generate, model, unique_notes, max_generated, seq_len)\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_generated\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'genrt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtest_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mpredicted_note\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mrandom_note_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munique_notes\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredicted_note\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mgenerate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_note_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1165\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m           \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1167\u001b[0;31m           callbacks=callbacks)\n\u001b[0m\u001b[1;32m   1168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1169\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3215\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3216\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3217\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3218\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n\u001b[1;32m   3219\u001b[0m                                  [x.numpy() for x in outputs])\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    556\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m    557\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m--> 558\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;31m# Only need to override the gradient in graph mode and when we have outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_register_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args)\u001b[0m\n\u001b[1;32m    413\u001b[0m             attrs=(\"executor_type\", executor_type,\n\u001b[1;32m    414\u001b[0m                    \"config_proto\", config),\n\u001b[0;32m--> 415\u001b[0;31m             ctx=ctx)\n\u001b[0m\u001b[1;32m    416\u001b[0m       \u001b[0;31m# Replace empty list with None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     59\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"fUGfKlsefv00","colab_type":"text"},"source":["# Making Attention Plot Graph"]},{"cell_type":"code","metadata":{"id":"miKl7ys2n6gC","colab_type":"code","colab":{}},"source":["# function for plotting the attention weights, taken from MIT course\n","# https://github.com/aamini/introtodeeplearning_labs\n","def plot_attention(attention, sentence, predicted_sentence):\n","    fig = plt.figure(figsize=(10,10))\n","    ax = fig.add_subplot(1, 1, 1)\n","    ax.matshow(attention, cmap='viridis')\n","    \n","    fontdict = {'fontsize': 14}\n","    \n","    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n","    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n","\n","    plt.show()\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XOv3jwhF8HSc","colab_type":"code","outputId":"f63b056a-a6c6-4ca1-9fc8-c1cc0b040f48","executionInfo":{"status":"ok","timestamp":1582534251768,"user_tz":-540,"elapsed":1276,"user":{"displayName":"아르떼","photoUrl":"","userId":"02010724767235108618"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["start_sequence = 150\n","seq_len = 50\n","\n","def visualize_attention(model, start_sequence, seq_len):\n","  \"\"\" This is where we utilize the eager execution of Tensorflow 2.\n","  \n","  \"\"\"\n","  test_input = np.array([generate])[:,start_sequence:start_sequence+seq_len]\n","  out = test_input\n","\n","  for i, layer in enumerate(model.layers):\n","    out = layer(out)\n","    if i == 3 or i == 6:\n","      print(\"Attention {}\".format(i//3))\n","      attention = out[1][0].numpy()\n","      trans_input = test_input[0].astype(str).tolist()\n","      plot_attention(attention, trans_input, trans_input)\n","      out = out[0]\n","      print()\n","\n","visualize_attention(model, start_sequence, seq_len)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Attention 1\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAlgAAAJcCAYAAAAl99rKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwaszJqqhAAAAAElFTkSuQmCC\n","text/plain":["<Figure size 720x720 with 1 Axes>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","Attention 2\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAlgAAAJcCAYAAAAl99rKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n","text/plain":["<Figure size 720x720 with 1 Axes>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"S7kNbc7TikxN","colab_type":"code","colab":{}},"source":["attention[30]"],"execution_count":0,"outputs":[]}]}